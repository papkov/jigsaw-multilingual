{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(D.Dataset):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.dataset = np.load(fn)\n",
    "        self.x = self.dataset['x']\n",
    "        self.y = self.dataset['y']\n",
    "        \n",
    "    def process(self, x, y):\n",
    "        return x, y\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.x[i], self.y[i]\n",
    "        return self.process(x, y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = Dataset('../input/validation.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAugmentation:\n",
    "    def __init__(self, p=0.5, bos_token=0, pad_token=1, eos_token=2, dot_token=5):\n",
    "        self.bos_token = bos_token\n",
    "        self.pad_token = pad_token\n",
    "        self.eos_token = eos_token\n",
    "        self.dot_token = dot_token\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleSentences(BaseAugmentation):\n",
    "    \"\"\"\n",
    "    Shuffles sentences after splitting by dot token\n",
    "    \"\"\"\n",
    "        \n",
    "    def __call__(self, sent):\n",
    "        new_sent = sent.copy()\n",
    "        \n",
    "        if np.random.uniform() < self.p:\n",
    "            # Mask meaningful part of a sentence\n",
    "            mask = (sent != self.bos_token) & (sent != self.pad_token) & (sent != self.eos_token)\n",
    "            # Localize dots\n",
    "            # TODO: a smatrer way to split?\n",
    "            dots = np.where(sent == self.dot_token)[0]\n",
    "            # Split by dot localization, shuffle\n",
    "            split = np.split(sent[mask], dots)\n",
    "            np.random.shuffle(split)\n",
    "            # Concatenate back and paste\n",
    "            new_sent[mask] = np.concatenate(split)\n",
    "        \n",
    "        return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = ShuffleSentences(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.8 µs ± 5.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _ = shuffle(valid[12][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> No sabemos quien es el autor del pie de imagen por ende no podemos acusarlo de haber inventado o no inventado la afirmacion, ergo no podemos decir que es investigacion original ya que no sabemos de donde obtuvo las lineas que escribió, lo que si sabemos es que la carga de la prueba recae sobre quien hace la afirmación, y en este caso es Xana la que afirma que el pie de imagen es subjetivo, pero, ¿segun quien?¿segun ella misma?, entonces en ese caso decir que el pie de iamgen es subjetivo es una afirmacion subjetiva de la propia subjetividad de Xana ya que no ha demostrado pruebas objetivas de que la susodicha sea realmente subjetiva, ergo eliminar tanto el pie de foto como la imagen misma nomás porque así lo dice la subjetividad de Xana, eso sería precisamente una edición subjetiva carente de argumentacion enciclopédica y por eso procedí a revertirla.Mr.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(shuffle(valid[12][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> No sabemos quien es el autor del pie de imagen por ende no podemos acusarlo de haber inventado o no inventado la afirmacion, ergo no podemos decir que es investigacion original ya que no sabemos de donde obtuvo las lineas que escribió, lo que si sabemos es que la carga de la prueba recae sobre quien hace la afirmación, y en este caso es Xana la que afirma que el pie de imagen es subjetivo, pero, ¿segun quien?¿segun ella misma?, entonces en ese caso decir que el pie de iamgen es subjetivo es una afirmacion subjetiva de la propia subjetividad de Xana ya que no ha demostrado pruebas objetivas de que la susodicha sea realmente subjetiva, ergo eliminar tanto el pie de foto como la imagen misma nomás porque así lo dice la subjetividad de Xana, eso sería precisamente una edición subjetiva carente de argumentacion enciclopédica y por eso procedí a revertirla.Mr.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(valid[12][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peenv (conda)",
   "language": "python",
   "name": "peenv36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
