{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from importlib import reload\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the toxicity label diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/jigsaw-toxic-comment-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_types_tr = ['severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "toxic_types_tr.sort()\n",
    "fig, ax = plt.subplots(facecolor='white')\n",
    "(train[toxic_types_tr].sum()/len(train)*100).sort_values().plot(kind='barh', ax=ax)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.ylabel('Percentage %')\n",
    "plt.xlabel('Toxicity type')\n",
    "plt.title('Distribution of Toxic Comments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ub = pd.read_csv('../input/jigsaw-unintended-bias-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ub[['toxic', 'rating']].head() # check what is the relevance of toxic and rating\n",
    "train_ub[['toxic', 'funny', 'wow','sad', 'likes', 'disagree']].groupby('toxic').sum().query('likes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ub_columns = np.array(train_ub.columns)\n",
    "train_ub_columns[train_ub_columns == 'identity_attack'] = 'identity_hate'\n",
    "train_ub_columns[train_ub_columns == 'severe_toxicity'] = 'severe_toxic'\n",
    "train_ub.columns = train_ub_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toxic_types_tr_ub = ['severe_toxic', 'obscene',\n",
    "       'identity_hate', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
    "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
    "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
    "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
    "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
    "       'other_sexual_orientation', 'physical_disability',\n",
    "       'psychiatric_or_mental_illness', 'transgender', 'white', 'sexual_explicit']\n",
    "toxic_types_tr_ub.sort()\n",
    "\n",
    "common_cols = []\n",
    "for col in toxic_types_tr_ub:\n",
    "    if col in toxic_types_tr:\n",
    "        common_cols.append(col)\n",
    "common_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ub.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ub[toxic_types_tr_ub].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# FIXME: this plot is not accurate as the instances have multiple labels.\n",
    "fig, ax = plt.subplots(facecolor='white')\n",
    "df = (train_ub[toxic_types_tr_ub].astype('bool').sum()/len(train_ub)*100).sort_values()\n",
    "ax = df.plot(kind='barh', figsize=(10,15), ax=ax)\n",
    "for i, tag in enumerate(df.index):\n",
    "    if tag in common_cols:\n",
    "        ax.patches[i].set_color('red')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.ylabel('Percentage %')\n",
    "plt.xlabel('Toxicity type')\n",
    "plt.title('Distribution of Toxic Comments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_instance_count = (train_ub[toxic_types_tr_ub].sum(axis=1) > 1).sum()\n",
    "print(f\"number of multilabel instance: {multilabel_instance_count}\")\n",
    "print(f\"total number of instance in the train_ub: {len(train_ub)}\")\n",
    "print(f\"percentage of instances with multiple labels: {multilabel_instance_count / len(train_ub) * 100:2.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: this plot is not accurate as the instances have multiple labels.\n",
    "fig, ax = plt.subplots(facecolor='white')\n",
    "toxic_types_counts = pd.DataFrame({'train': train[toxic_types_tr].sum(), 'train_ub': train_ub[common_cols].sum()})\n",
    "toxic_types_counts.plot(kind='bar', ax=ax)\n",
    "plt.yscale('log')\n",
    "plt.grid(alpha=0.5)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Toxicity type')\n",
    "plt.title('Distribution of Toxic Comments')\n",
    "plt.hlines(len(train), -10, 10, label='train size', color='blue', linestyle='--')\n",
    "plt.hlines(len(train_ub), -10, 10, label='train_ub size', color='orange', linestyle='--')\n",
    "plt.ylim([0,2.5e6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations:\n",
    "# train_ub has different proportion of toxic comments that might influence our generalization on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tmp = pd.concat([train, pd.Series(np.ones(len(train)), name='toxic_and_non_toxic')], axis=1)\n",
    "train_tmp['toxic_binary'] = train_tmp.apply(lambda row: row['toxic'] > 0.5, axis=1) # if toxic is binary yields to same value\n",
    "train_tmp_count = train_tmp.groupby(toxic_types_tr).sum()\n",
    "train_tmp_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_tmp_count['toxic'] / train_tmp_count['toxic_and_non_toxic']).plot(kind='bar', figsize=(17,3))\n",
    "plt.title('proportion of each toxicity type with the total instances with same label') # that might be classified as toxic/non-toxic\n",
    "plt.grid(alpha=0.25)\n",
    "plt.show()\n",
    "# conclusion: there are some instances that have a toxicity label but were not identified as toxic\n",
    "# (perhaps their) toxicity was lower than 0.5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_tmp_count['toxic'].sum() / train.query('toxic >= 0.5').shape[0] == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_types_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_tmp_count['toxic'] / train.query('toxic >= 0.5').shape[0]) \\\n",
    "                                .sort_values(ascending=False).plot(kind='bar', figsize=(17,3))\n",
    "plt.title('Proportion of each toxicity type with the total instance with same label and classified as toxic (>0.5)')\n",
    "plt.grid(alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "# observation: \n",
    "#  ~35% with none of the labels is considered as toxic\n",
    "#  ~27% with insult and obscene\n",
    "#  ~12% with severe_toxic\n",
    "#  ~8% only insult \n",
    "#  ~5% insult, obscene and severe_toxic\n",
    "#  ~5% identity_hate, insult, and obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of instance in train:  {len(train):8}\")\n",
    "print(f\"number of instance in train_ub: {len(train_ub):7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the language diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.detect import Detector\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[0-9\"]', '', text)\n",
    "    text = re.sub(r'#[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'@[\\S]+\\b', '', text)\n",
    "    text = re.sub(r'https?\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def lang_detector(text):\n",
    "    text = clean_text(text)\n",
    "    rslt = Detector(\"\".join(x for x in text if x.isprintable()), quiet=True)\n",
    "    return rslt.language.code, rslt.language.confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = train['comment_text'].apply(lang_detector)\n",
    "langs = pd.DataFrame(list(langs), columns=['lang', 'confidence'])\n",
    "train = pd.concat([train, langs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "langs_ub = train_ub['comment_text'].apply(lang_detector)\n",
    "langs_ub = pd.DataFrame(list(langs_ub), columns=['lang', 'confidence'])\n",
    "train_ub = pd.concat([train_ub, langs_ub], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_count(train_data):\n",
    "    lang_list = sorted(list(set(train_data[\"lang\"])))\n",
    "    counts = [list(train_data[\"lang\"]).count(cont) for cont in lang_list]\n",
    "    df = pd.DataFrame(np.transpose([lang_list, counts]))\n",
    "    df.columns = [\"Language\", \"Count\"]\n",
    "    df[\"Count\"] = df[\"Count\"].apply(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_vs_non_english(df_count):\n",
    "    df_en = pd.DataFrame({'English: ': df_count.query(\"Language == 'en'\").Count,\n",
    "                          'non-English': df_count.query(\"Language != 'en' and Language != 'un'\").Count.sum()}, )\n",
    "    df_en = df_en.T\n",
    "    df_en.columns = ['Count'] # use a better way to create df_en\n",
    "    df_en.sort_values(by='Count').plot(kind='barh')\n",
    "    # plt.xscale('log')\n",
    "    plt.grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_langs(df_count):\n",
    "    # fig, axes= plt.subplots(3,1, figsize=(17,10))\n",
    "    # df_count.query(\"Language != 'en' and Language != 'un'\").query(\"Count >= 20\") \\\n",
    "    #             .set_index('Language').sort_values(by='Count', ascending=False).plot(kind='bar', ax=axes[0])\n",
    "    # axes[0].grid(alpha=0.2)\n",
    "    # df_count.query(\"Language != 'en' and Language != 'un'\").query(\"Count < 20 and Count >= 10\") \\\n",
    "    #             .set_index('Language').sort_values(by='Count', ascending=False).plot(kind='bar', ax=axes[1])\n",
    "    # plt.subplots_adjust(hspace=0.5)\n",
    "    # axes[1].grid(alpha=0.2)\n",
    "    # df_count.query(\"Language != 'en' and Language != 'un'\").query(\"Count < 10\") \\\n",
    "    #             .set_index('Language').sort_values(by='Count', ascending=False).plot(kind='bar', ax=axes[2])\n",
    "    # plt.subplots_adjust(hspace=0.5)\n",
    "    # axes[2].grid(alpha=0.2)\n",
    "    # plt.show()\n",
    "\n",
    "    df_count.query(\"Language != 'en' and Language != 'un'\").query(\"Count >= 50\") \\\n",
    "                .set_index('Language').sort_values(by='Count', ascending=True).iloc[-8:].plot(kind='barh')\n",
    "    plt.grid(alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count = language_count(train)\n",
    "english_vs_non_english(df_count)\n",
    "other_langs(df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# some cleaning might be useful based on unknown language, or language detected but with low confidence ..\n",
    "# ..(especially when lang code is used as input to the model)\n",
    "# \n",
    "# train.query(\"lang == 'en' and confidence < 80\")\n",
    "# train_ub.query(\"lang == 'en' and confidence < 80 and toxic > 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_count_ub = language_count(train_ub)\n",
    "english_vs_non_english(df_count_ub)\n",
    "other_langs(df_count_ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_ub = language_count(train_ub)\n",
    "english_vs_non_english(df_count_ub)\n",
    "other_langs(df_count_ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.query(\"Language == 'un'\").Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# perhaps better to remove the languages that are unknown or detected with low confidence from the train data\n",
    "# perhaps it is good to check the language detection output with the lang tag in validation and test..\n",
    "#  .. to understand any potential discrepencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([\n",
    "    train[['comment_text', 'toxic']],\n",
    "    train_ub[['comment_text', 'toxic']].query('toxic==1'),\n",
    "    train_ub[['comment_text', 'toxic']].query('toxic==0').sample(n=150000, random_state=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = train_all['comment_text'].apply(lang_detector)\n",
    "langs = pd.DataFrame(list(langs), columns=['lang', 'confidence'])\n",
    "train_all = pd.concat([train_all.reset_index(drop=True), langs.reset_index(drop=True)], axis=1)\n",
    "#\n",
    "df_count = language_count(train_all)\n",
    "english_vs_non_english(df_count)\n",
    "other_langs(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study the validation and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = pd.read_csv('../input/validation.csv', index_col=0)\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test.csv', index_col=0)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_va = language_count(valid).set_index('Language')\n",
    "df_count_te = language_count(test).set_index('Language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_valid_test = pd.concat([df_count_te, df_count_va], axis=1,)\n",
    "df_count_valid_test.columns = ['Test', 'Valid']\n",
    "df_count_valid_test.sort_values(by='Test').plot(kind='bar')\n",
    "plt.grid(alpha=0.25)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Language')\n",
    "plt.title('Distribution of Languages in Validation and Test')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:basenv]",
   "language": "python",
   "name": "conda-env-basenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
